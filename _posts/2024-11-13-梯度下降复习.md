# 梯度下降复习（大白话版）

当损失函数的梯度不能直接求出解析解时，我们通常会使用梯度下降（Gradient Descent）或其他数值优化方法来找到参数的最优值。以下是梯度下降方法的基本原理和步骤：

1. **梯度方向**：梯度向量 $∇θ_L$ 指示了损失函数 $L$在参数空间中增长最快的方向。因此，沿着梯度的反方向移动可以减少损失函数的值。
2. **更新参数**：在每次迭代中，我们根据梯度的方向和大小更新参数。更新规则如下： $θ_{new}=θ_{old}−η⋅∇θ_L*$ 其中$η$ 是学习率，它控制我们在梯度方向上移动的步长。
3. **学习率控制**：学习率 $η$ 是一个关键的超参数，它决定了每次更新参数时的步长。如果学习率太大，可能会导致在最小值附近震荡，甚至发散；如果学习率太小，可能会导致收敛速度过慢。
4. **迭代更新**：重复计算梯度和更新参数的过程，直到满足某个停止条件，比如梯度足够小、达到最大迭代次数或损失函数的值低于某个阈值。
5. **逼近最优解**：通过不断迭代，参数会逐渐逼近损失函数的最小值点。

梯度下降方法的关键在于：

- **方向**：梯度提供了减少损失的方向。
- **步长**：学习率控制了在梯度方向上更新参数的步长。

通过这种方法，即使不能直接求出解析解，我们也可以逐步逼近参数的最优值。此外，还有许多梯度下降的变体，如动量（Momentum）、AdaGrad、RMSprop、Adam等，它们通过不同的方式改进了梯度下降的性能和稳定性。